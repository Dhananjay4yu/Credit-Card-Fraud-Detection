# ğŸ’³ Credit Card Fraud Detection

![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)
![Scikit-Learn](https://img.shields.io/badge/ML-Scikit--Learn-orange?logo=scikit-learn)
---

## ğŸ“Œ Project Overview  
Credit card fraud is one of the biggest challenges in the financial sector.  
This project demonstrates an **end-to-end pipeline** for fraud detection using:  

âœ… Exploratory Data Analysis (EDA)  
âœ… Data preprocessing & feature scaling  
âœ… Handling imbalanced data (SMOTE, Oversampling, Undersampling, ADASYN)  
âœ… Building & tuning multiple ML models  
âœ… Evaluating performance with **ROC-AUC, Precision, Recall, F1-score**  
âœ… Business-oriented cost-benefit analysis  

---

## ğŸ“Š Exploratory Data Analysis (EDA)  
- Dataset is **PCA-transformed**, so no additional outlier treatment needed.  
- Only `Amount` column required **feature scaling**.  
- Skewness mitigated with **PowerTransformer** â†’ variables became normally distributed.  
- Severe class imbalance observed (only **0.17% fraudulent transactions**).  

---

## âš™ï¸ Data Preprocessing  
- **Scaling:** Applied only on `Amount`.  
- **No leakage:** Fit scaler only on **train**, transform **test**.  
- **Balancing techniques used:**  
  - Undersampling  
  - Oversampling  
  - SMOTE  
  - ADASYN  

---

## ğŸ“ˆ Model Building & Evaluation  

### ğŸ” Algorithms Explored
- **Baseline:** Logistic Regression, Decision Tree  
- **Ensemble:** Random Forest, XGBoost  

âŒ **Not Used:**  
- **SVM** â†’ Too computationally expensive on large datasets.  
- **KNN** â†’ Memory-inefficient and slow for large datasets.  

### ğŸ”§ Hyperparameter Tuning  
- **Grid Search + K-Fold CV**
- **OPTUNA based tuning- uses Baysean search**
- Tuned key params like **C (Logistic Regression)**, tree depth, learning rates, etc.  

### ğŸ“Š Metrics Used
- ROC-AUC (primary)  
- F1-Score  
- Precision & Recall  
- Confusion Matrix  

---

## ğŸ† Model Performance  

**Example (SMOTE + XGBOOST):**  
- **Accuracy:** `0.9993`  
- **Sensitivity (Recall):** `0.6591`  
- **Specificity:** `0.9999`  
- **F1-Score:** `0.7688`  
- **ROC-AUC:** ~`0.97` (test set)  

---

## ğŸ… Best Model Selection  
- **Undersampling** â†’ Good, but lost valuable info.  
- **SMOTE & ADASYN** â†’ Produced strong, balanced results.  

âœ… **Best Choice:** **Logistic Regression + SMOTE**  
- Train ROC-AUC: `0.99` | Test ROC-AUC: `0.97`  
- High Recall â†’ catches fraudulent transactions effectively  
- Simple, interpretable, low-resource model  

---

## ğŸ’¡ Business Insights  

### Small Transaction Banks ğŸ¦  
- Need **high precision** â†’ fewer false fraud alerts â†’ reduces manual verification cost  

### Large Transaction Banks ğŸ’°  
- Need **high recall** â†’ avoid missing high-value fraud cases  

### Cost-Benefit Tradeoff  
- Complex models (Random Forest, XGBoost) = high infra cost ğŸš¨  
- Logistic Regression = efficient, interpretable, cost-effective âœ…  

---
## âœ¨ Future Work  
- Try **Deep Learning (Autoencoders, LSTMs)** for anomaly detection  
- Deploy as **real-time API** for banks  
- Explore **cost-sensitive learning**  

